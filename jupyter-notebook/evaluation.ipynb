{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "sns.set(style=\"white\") # reset font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets: Number of features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset properties file\n",
    "dataset_properties = pd.read_csv(\"../results/datasets/classification_binary_datasets.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = dataset_properties[\"NumberOfFeatures\"].values\n",
    "\n",
    "ax = sns.displot(number_of_features, binwidth=500)\n",
    "ax.set(xlabel='Number of Features', ylabel='Number of datasets')\n",
    "ax.savefig('dataset_feature_distribution.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iteration Experiment Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_experiment_path = \"../results/iteration_number_experiment/\"\n",
    "iter_experiment_output_path = \"iter_experiment_plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all iteration data\n",
    "all_iteration_data = pd.DataFrame()\n",
    "for root, dirs, files in os.walk(iter_experiment_path):\n",
    "    for filename in files:\n",
    "        if filename != \".gitignore\":\n",
    "            all_iteration_data = pd.concat([all_iteration_data, pd.read_csv(iter_experiment_path + filename, skiprows=[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Convergence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print convergence plot of training and testing timeseries\n",
    "def printAllIterationsTestingScore(name, group):\n",
    "    # group values for each approach\n",
    "    gb = group.groupby([\"Dataset ID\", \"Estimator\", \"Metric\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\"], as_index=False)\n",
    "    grouped_data = [gb.get_group(x) for x in gb.groups]\n",
    "    \n",
    "    fig, ax = plt.subplots(5, 2, figsize=(15,20))\n",
    "    \n",
    "    ax[0,0].set_xlabel('Number of Iterations', size=15) \n",
    "    ax[0,0].set_ylabel('Score', size=15)\n",
    "\n",
    "    y_min = 1\n",
    "    y_max = 0\n",
    "    \n",
    "    for index in range(0,len(grouped_data)):\n",
    "        if index < 2:\n",
    "            i=0\n",
    "            j=index\n",
    "        elif index < 4:\n",
    "            i=1\n",
    "            j=index-2\n",
    "        elif index < 6:\n",
    "            i=2\n",
    "            j=index-4\n",
    "        elif index < 8:\n",
    "            i=3\n",
    "            j=index-6\n",
    "        else:\n",
    "            i=4\n",
    "            j=index-8\n",
    "            \n",
    "        iteration_labels = grouped_data[index][\"Iteration Steps\"]\n",
    "        testing_values = grouped_data[index][\"Testing Score\"]\n",
    "        training_values = grouped_data[index][\"Training Score\"]\n",
    "        ax[i,j].plot(iteration_labels, testing_values, label = \"Testing\")\n",
    "        ax[i,j].plot(iteration_labels, training_values, label = \"Training\")\n",
    "        discr_method = grouped_data[index].loc[:, \"Discretization Method\"].values[0]\n",
    "        learning_method = grouped_data[index].loc[:, \"Learning Method\"].values[0]\n",
    "        kernel = grouped_data[index].loc[:, \"Kernel\"].values[0]\n",
    "        \n",
    "        if learning_method == \"RF\":\n",
    "            title = grouped_data[index].loc[:, \"Learning Method\"].values[0] + \"-\" + grouped_data[index].loc[:, \"Discretization Method\"].values[0]\n",
    "        else:\n",
    "            title = grouped_data[index].loc[:, \"Learning Method\"].values[0] + \"-\" + grouped_data[index].loc[:, \"Kernel\"].values[0] + \"-\" + grouped_data[index].loc[:, \"Discretization Method\"].values[0]\n",
    "        \n",
    "        ax[i,j].set_title(title, size=20)\n",
    "        ax[i,j].tick_params(labelsize=20)\n",
    "        ax[i,j].legend()\n",
    "        # set y axis range to the same for both subplots\n",
    "        y_min = min(ax[i,j].get_ylim()[0], y_min)\n",
    "        y_max = max(ax[i,j].get_ylim()[1], y_max)\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    # set y axis range\n",
    "    for i, axis in enumerate(ax.flat):\n",
    "        axis.set_ylim(y_min, y_max)\n",
    "        \n",
    "    return fig\n",
    "\n",
    "grouped_iteration_data_convergence = all_iteration_data.groupby([\"Dataset ID\", \"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in grouped_iteration_data_convergence:\n",
    "    # plot only dataset 312\n",
    "    if name[0] == 312:\n",
    "        iter_plot = printAllIterationsTestingScore(name, group)\n",
    "        # save plot to file\n",
    "        filename = str(name[0]) + \"_\" + name[1] + \"_\" + name[2] + \".pdf\"\n",
    "        iter_plot.savefig(iter_experiment_output_path + \"convergence/\"+ filename, format=\"pdf\", bbox_inches = \"tight\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print correlation boxplot of training and testing timeseries\n",
    "def getCorrelationBetweenTestingTraining(name_estimator_metric, group_estimator_metric):\n",
    "    grouped_approach = group_estimator_metric.groupby([\"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"Number Features\"])\n",
    "    corr_values = {}\n",
    "    \n",
    "    for name_approach, group_approach in grouped_approach:\n",
    "        if name_approach[0] == \"RF\":\n",
    "            label = name_approach[0]+\"-\"+name_approach[2]\n",
    "        else:\n",
    "            label = name_approach[0]+\"-\"+name_approach[1]+\"-\"+name_approach[2]\n",
    "        corr_values[label] = {}\n",
    "        grouped_dataset = group_approach.groupby([\"Dataset ID\"])\n",
    "        for name_dataset, group_dataset in grouped_dataset:\n",
    "            corr_values[label][name_dataset] = group_dataset[\"Testing Score\"].corr(group_dataset[\"Training Score\"])\n",
    "    df = pd.DataFrame(corr_values)\n",
    "    \n",
    "    # filter NaN's\n",
    "    mask =~ np.isnan(df)\n",
    "    filtered_list = []\n",
    "    for (i1,d), (i2,m) in zip(df.T.iterrows(), mask.T.iterrows()):\n",
    "        if i1 != i2:\n",
    "             raise ValueError(\"Wrong indices\")\n",
    "        filtered_list.append(d.values[m])\n",
    "\n",
    "    return filtered_list, df.columns\n",
    "\n",
    "# create boxplot of average correlation\n",
    "grouped_iteration_data_correlation = all_iteration_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "data_dict = {}\n",
    "for name, group in grouped_iteration_data_correlation:\n",
    "    data_dict[name] = getCorrelationBetweenTestingTraining(name, group)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.boxplot(data_dict[list(data_dict.keys())[0]][0])\n",
    "ax1.set_xticklabels(data_dict[list(data_dict.keys())[0]][1])\n",
    "title1 = list(data_dict.keys())[0][0] + \"-\" + list(data_dict.keys())[0][1]\n",
    "ax1.set_title(title1, fontsize=15)\n",
    "ax1.set_ylabel(\"Correlation\", size=15)\n",
    "ax1.tick_params(labelsize=15)\n",
    "ax1.set_ylim(-1,1)\n",
    "ax2.boxplot(data_dict[list(data_dict.keys())[1]][0])\n",
    "ax2.set_xticklabels(data_dict[list(data_dict.keys())[1]][1])\n",
    "title2 = list(data_dict.keys())[1][0] + \"-\" + list(data_dict.keys())[1][1]\n",
    "ax2.set_title(title2, fontsize=15)\n",
    "ax2.tick_params(labelsize=15)\n",
    "ax2.set_ylim(-1,1)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "filename = \"iter_correlation_boxplot.pdf\"\n",
    "fig.savefig(iter_experiment_output_path + \"correlation/\" + filename, format=\"pdf\", bbox_inches = \"tight\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the result csv files are stored\n",
    "comparison_experiment_path = \"../results/comparison_bayesian_experiment/classification/\"\n",
    "comparison_experiment_output_path = \"comparison_experiment_plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available data\n",
    "dataset_set = set() # set of all dataset id, estimator and metric combinations\n",
    "filename_list = []\n",
    "\n",
    "# explore folder and store all dataset ids and filenames\n",
    "file_list = os.listdir(comparison_experiment_path)\n",
    "for file in file_list:\n",
    "    if file.startswith(\"bayopt\") or file.startswith(\"comparison\") or file.startswith(\"withoutfs\"):\n",
    "        # get properties\n",
    "        dataset_id = file.split(\"_\")[1]\n",
    "        estimator = file.split(\"_\")[2] + \"_\" + file.split(\"_\")[3]\n",
    "        metric = file.split(\"_\")[4].split(\".\")[0]\n",
    "        dataset_set.add((dataset_id, estimator, metric))\n",
    "        filename_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate all bayesian, comparison and withoutfs into one dataframe\n",
    "all_bayopt_data = pd.DataFrame()\n",
    "all_comparison_data = pd.DataFrame()\n",
    "all_withoutfs_data = pd.DataFrame()\n",
    "\n",
    "for dataset_id, estimator, metric in dataset_set:\n",
    "    path_suffix = dataset_id + \"_\" + estimator + \"_\" + metric + \".csv\"\n",
    "    \n",
    "    # import bay opt\n",
    "    bayopt_import= pd.read_csv(comparison_experiment_path + \"bayopt_\" + path_suffix, skiprows=[1])\n",
    "    bayopt_import[\"did\"] = dataset_id\n",
    "    bayopt_import[\"Estimator\"] = estimator\n",
    "    bayopt_import[\"Metric\"] = metric\n",
    "    bayopt_import[\"did\"] = pd.to_numeric(bayopt_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    # import comparison\n",
    "    comparison_import = pd.read_csv(comparison_experiment_path + \"comparison_\" + path_suffix, skiprows=[1])\n",
    "    comparison_import[\"did\"] = dataset_id\n",
    "    comparison_import[\"Estimator\"] = estimator\n",
    "    comparison_import[\"Metric\"] = metric\n",
    "    comparison_import[\"did\"] = pd.to_numeric(comparison_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    # import withoutfs\n",
    "    withoutfs_import = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix)\n",
    "    if withoutfs_import.iloc[0,0] == \"mean\":\n",
    "        withoutfs_import = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix, skiprows=[1])\n",
    "    withoutfs_import[\"did\"] = dataset_id\n",
    "    withoutfs_import[\"Estimator\"] = estimator\n",
    "    withoutfs_import[\"Metric\"] = metric\n",
    "    withoutfs_import[\"did\"] = pd.to_numeric(withoutfs_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    all_bayopt_data = pd.concat([all_bayopt_data, bayopt_import])\n",
    "    all_comparison_data = pd.concat([all_comparison_data, comparison_import])\n",
    "    all_withoutfs_data = pd.concat([all_withoutfs_data, withoutfs_import])\n",
    "\n",
    "# replace strings\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Gaussian Process\", \"GP\")\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Random Forest\", \"RF\")\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Radial Basis Functions\", \"RBF\")\n",
    "all_comparison_data = all_comparison_data.replace(\"Recursive Feature Selection\", \"RFE\")\n",
    "all_comparison_data = all_comparison_data.replace(\"Sequential Feature Selection\", \"SFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. BOFS scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot with all final scores\n",
    "def createScoreBoxPlot(bayopt_data):\n",
    "    all_bayopt_data_grouped_estimator = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    for name_estimator, group_estimator in all_bayopt_data_grouped_estimator:\n",
    "        all_bayopt_data_grouped = group_estimator.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "        data_dict[name_estimator[0] + \"-\" + name_estimator[1]] = {}\n",
    "        for name, group in all_bayopt_data_grouped:\n",
    "            if name[1] == \"RF\":\n",
    "                label = name[1] + \"-\" + name[3]\n",
    "            else:\n",
    "                label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "            value = group[\"Testing Score\"]\n",
    "            data_dict[name_estimator[0] + \"-\" + name_estimator[1]][label] = value\n",
    "    \n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.boxplot(data_df[data_df.columns[0]])\n",
    "    ax1.set_ylabel(\"Score\", size=15)\n",
    "    ax1.set_xticklabels(data_df.index)\n",
    "    ax1.set_title(data_df.columns[0], fontsize=15)\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax2.boxplot(data_df[data_df.columns[1]])\n",
    "    ax2.set_xticklabels(data_df.index)\n",
    "    ax2.set_title(data_df.columns[1], fontsize=15)\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "average_score_chart = createScoreBoxPlot(all_bayopt_data)\n",
    "average_score_chart.savefig(comparison_experiment_output_path + \"score_bofs/score_boxplot.pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOFS rank boxplot\n",
    "def createBOFSRankBoxplot(bayopt_data):\n",
    "    all_bayopt_data_grouped_estimator = bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "    \n",
    "    # init dict\n",
    "    data_dict = dict()\n",
    "    for name_estimator, group_estimator in all_bayopt_data_grouped_estimator:\n",
    "        label_estimator = name_estimator[0] + \"-\" + name_estimator[1]\n",
    "        data_dict[label_estimator] = {}\n",
    "        grouped_approaches = group_estimator.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "        for name_approach, _ in grouped_approaches:\n",
    "            label_approaches = name_approach[1] + \"-\" + name_approach[2] + \"-\" + name_approach[3]\n",
    "            data_dict[label_estimator][label_approaches] = []\n",
    "    \n",
    "    # calculate ranks\n",
    "    for name_estimator, group_estimator in all_bayopt_data_grouped_estimator:\n",
    "        all_bayopt_data_grouped_did = group_estimator.groupby([\"did\"])\n",
    "        estimator_label = name_estimator[0] + \"-\" + name_estimator[1]\n",
    "        for name_did, group_did in all_bayopt_data_grouped_did:\n",
    "            sorted_group = group_did.sort_values([\"Testing Score\"], ascending=False).reset_index(drop=True)\n",
    "            for index, row in sorted_group.iterrows():\n",
    "                row_label = row[1] + \"-\" + row[2] + \"-\" + row[3]\n",
    "                data_dict[estimator_label][row_label].append(index+1)\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    labels = []\n",
    "    for x in df.index:\n",
    "        if x == \"RF---Categorical\":\n",
    "            labels.append(\"RF-Categorical\")\n",
    "        else:\n",
    "            labels.append(x)\n",
    "    \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.boxplot(df[df.columns[0]])\n",
    "    ax1.set_title(df.columns[0], fontsize=15)\n",
    "    ax1.set_ylim(0.5,10.5)\n",
    "    ax1.set_yticks(np.arange(1, 11, 1.0))\n",
    "    ax1.set_xticklabels(labels)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax1.set_ylabel('Rank', size=15)\n",
    "    ax1.invert_yaxis()\n",
    "    ax2.boxplot(df[df.columns[1]])\n",
    "    ax2.set_title(df.columns[1], fontsize=15)\n",
    "    ax2.set_ylim(0.5,10.5)\n",
    "    ax2.set_yticks(np.arange(1, 11, 1.0))\n",
    "    ax2.set_xticklabels(labels)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "rank_boxplot = createBOFSRankBoxplot(all_bayopt_data)\n",
    "rank_boxplot.savefig(comparison_experiment_output_path + \"score_bofs/rank_box_plot.pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score correlation heatmap of BOFS approaches\n",
    "def createCorrelationMatrixOnlyBayopt(bayopt_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    for name, group in grouped_bayopt_df:\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        data_dict[label] = scores\n",
    "    \n",
    "    df = pd.DataFrame(data_dict)\n",
    "    correlation_df = df.corr()\n",
    "    \n",
    "    # display only lower diagonal values\n",
    "    mask = np.zeros_like(correlation_df, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    mask[np.diag_indices_from(mask)] = False\n",
    "    \n",
    "    sns.set(style=\"white\", font_scale=1.3)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax = sns.heatmap(data=correlation_df, mask=mask, vmin=0.5, vmax=1, center=0.75, cmap='RdYlGn', annot=True, square=True, cbar=False)\n",
    "    sns.set(style=\"white\")\n",
    "    return ax\n",
    "\n",
    "for name, group in all_bayopt_data.groupby([\"Estimator\", \"Metric\"]):\n",
    "    heatmap_only_bayopt = createCorrelationMatrixOnlyBayopt(group)\n",
    "    figure = heatmap_only_bayopt.get_figure()\n",
    "    figure.savefig(comparison_experiment_output_path + \"score_bofs/score_bayopt_correlation_heatmap_\" + name[0] + \"_\" + name[1] + \".pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "    sns.set(style=\"white\") # reset font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. BOFS Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average runtime of BOFS approaches (including black-box-evaluation time and overhead time)\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "data = {}\n",
    "\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values_black_box = []\n",
    "    values_overhead = []\n",
    "    labels_bay_opt = []\n",
    "    data_dict = dict()\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values_black_box.append(sum(group[\"Duration Black Box\"])/len(group[\"Duration Black Box\"]))\n",
    "        values_overhead.append(sum(group[\"Duration Overhead\"])/len(group[\"Duration Overhead\"]))\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "    data[estimator + \"_\" + metric] =  (labels_bay_opt, values_black_box, values_overhead)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][2], label=\"Overhead Runtime\")\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][1], bottom=df[df.columns[0]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax1.set_ylabel(\"Average runtime [s]\", size=15)\n",
    "ax1.set_title(df.columns[0], size=15)\n",
    "ax1.legend()\n",
    "ax1.tick_params(labelsize=15)\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][2], label=\"Overhead Runtime\")\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][1], bottom=df[df.columns[1]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax2.set_title(df.columns[1], size=15)\n",
    "ax2.legend()\n",
    "ax2.tick_params(labelsize=15)\n",
    "# set y axis range to the same for both subplots\n",
    "y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "ax1.set_ylim(0, y_max)\n",
    "ax2.set_ylim(0, y_max)\n",
    "fig.autofmt_xdate()\n",
    "plt.savefig(comparison_experiment_output_path + \"runtime_plots/bofs_average_runtime.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average runtime per iteration of BOFS approaches (including black-box-evaluation time and overhead time)\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "data = {}\n",
    "\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values_black_box = []\n",
    "    values_overhead = []\n",
    "    labels_bay_opt = []\n",
    "    data_dict = dict()\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        avg_black_box_runtime = sum(group[\"Duration Black Box\"]) / len(group[\"Duration Black Box\"])\n",
    "        avg_overhead_runtime = sum(group[\"Duration Overhead\"])/len(group[\"Duration Overhead\"])\n",
    "        avg_iterations = sum(group[\"Number of Iterations\"]) / len(group[\"Number of Iterations\"])\n",
    "        values_black_box.append(avg_black_box_runtime/avg_iterations)\n",
    "        values_overhead.append(avg_overhead_runtime/avg_iterations)\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "    data[estimator + \"_\" + metric] =  (labels_bay_opt, values_black_box, values_overhead)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][2], label=\"Overhead Runtime\")\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][1], bottom=df[df.columns[0]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax1.set_ylabel(\"Avg. runtime per Iteration [s]\", size=15)\n",
    "ax1.set_title(df.columns[0], size=15)\n",
    "ax1.legend()\n",
    "ax1.tick_params(labelsize=15)\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][2], label=\"Overhead Runtime\")\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][1], bottom=df[df.columns[1]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax2.set_title(df.columns[1], size=15)\n",
    "ax2.legend()\n",
    "ax2.tick_params(labelsize=15)\n",
    "# set y axis range to the same for both subplots\n",
    "y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "ax1.set_ylim(0, y_max)\n",
    "ax2.set_ylim(0, y_max)\n",
    "fig.autofmt_xdate()\n",
    "plt.savefig(comparison_experiment_output_path + \"runtime_plots/bofs_average_runtime_per_iteration.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of number of iterations until convergence (50 iterations without new optimum) throughout the different datasets\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "res_dict = dict()\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values = []\n",
    "    labels = []\n",
    "    res_dict[estimator + \"-\" + metric] = {}\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values = group[\"Number of Iterations\"].values\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        res_dict[estimator + \"-\" + metric][label] = values\n",
    "\n",
    "df = pd.DataFrame(res_dict)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.set_title(df.columns[0], size=15)\n",
    "ax1.boxplot(df[df.columns[0]])\n",
    "ax1.set_xticklabels(df.index)\n",
    "ax1.set_ylabel(\"Iterations to convergence\", size=15)\n",
    "ax1.tick_params(labelsize=15)\n",
    "\n",
    "ax2.set_title(df.columns[1], size=15)\n",
    "ax2.boxplot(df[df.columns[1]])\n",
    "ax2.set_xticklabels(df.index)\n",
    "ax2.tick_params(labelsize=15)\n",
    "fig.autofmt_xdate()\n",
    "fig.savefig(comparison_experiment_output_path + \"runtime_plots/boxplot_bayesian_iterations.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. BOFS - state-of-the-art Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatse bar plot of scores of datasets 1468 and 1485\n",
    "def create_bar_plot_of_scores(bayopt_data, comparison_data, withoutfs_data):\n",
    "    \"\"\" Create plot with Bayesian optimization and comparison scores\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    bayopt_scores = bayopt_data[\"Testing Score\"].values\n",
    "    bayopt_labels = []\n",
    "    for index, row in bayopt_data.iterrows():\n",
    "        if row[\"Learning Method\"] == \"RF\":\n",
    "             bayopt_labels.append(row[\"Learning Method\"] + \" - \" + row[\"Discretization Method\"])\n",
    "        else:\n",
    "            bayopt_labels.append(row[\"Learning Method\"] + \" - \" + row[\"Kernel\"] + \" - \" + row[\"Discretization Method\"])\n",
    "    comparison_scores = comparison_data[\"Testing Score\"].values\n",
    "    comparison_labels = [row[\"Algorithm\"] + \" (\" + row[\"Approach\"] + \")\" for index, row in comparison_data.iterrows()]\n",
    "    withoutfs_score = withoutfs_data[\"Testing Score\"][0]\n",
    "    \n",
    "    ax1.bar(bayopt_labels, bayopt_scores)\n",
    "    ax1.set_title(\"BOFS\", fontsize=15)\n",
    "    ax1.set_ylabel(\"Score\", size=15)\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax1.axhline(y=withoutfs_score)\n",
    "    \n",
    "    ax2.bar(comparison_labels, comparison_scores)\n",
    "    ax2.set_title(\"Comparison\")\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    ax2.axhline(y=withoutfs_score)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "for dataset_id, estimator, metric in dataset_set:\n",
    "    if dataset_id == \"1468\" or dataset_id == \"1485\":\n",
    "        path_suffix = dataset_id + \"_\" + estimator + \"_\" + metric + \".csv\"\n",
    "\n",
    "        # create dataframes\n",
    "        bayopt_data = pd.read_csv(comparison_experiment_path + \"bayopt_\" + path_suffix, skiprows=[1])\n",
    "        comparison_data = pd.read_csv(comparison_experiment_path + \"comparison_\" + path_suffix , skiprows=[1])\n",
    "        withoutfs_data = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix)\n",
    "        if withoutfs_data.iloc[0,0] == \"mean\":\n",
    "            withoutfs_data = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix, skiprows=[1])\n",
    "\n",
    "        # replace strings\n",
    "        bayopt_data = bayopt_data.replace(\"Gaussian Process\", \"GP\")\n",
    "        bayopt_data = bayopt_data.replace(\"Random Forest\", \"RF\")\n",
    "        bayopt_data = bayopt_data.replace(\"Radial Basis Functions\", \"RBF\")\n",
    "        comparison_data = comparison_data.replace(\"Recursive Feature Selection\", \"RFE\")\n",
    "        comparison_data = comparison_data.replace(\"Sequential Feature Selection\", \"SFS\")\n",
    "\n",
    "        # create plot and save file\n",
    "\n",
    "        plot = create_bar_plot_of_scores(bayopt_data, comparison_data, withoutfs_data)\n",
    "        filename = dataset_id + \"_\" + estimator + \"_\" + metric + \".pdf\"\n",
    "        plot.savefig(comparison_experiment_output_path + \"score_comparison/\" + filename, format=\"pdf\", bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of final scores of BOFS and state-of-the-art approaches\n",
    "def createScoreComparisonBoxplot(bayopt_data, comparison_data, withoutfs_data):\n",
    "    data_dict = dict()\n",
    "    all_bayopt_data_grouped = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    all_comparison_data_grouped = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    \n",
    "    value_withoutfs = withoutfs_data[\"Testing Score\"]\n",
    "    values_bayopt = []\n",
    "    labels_bayopt = []\n",
    "    for name, group in all_bayopt_data_grouped:\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bayopt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bayopt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "        values_bayopt.append(group[\"Testing Score\"])\n",
    "\n",
    "    values_comparison = []\n",
    "    labels_comparison = []\n",
    "    # include withoutfs values\n",
    "    values_comparison.append(value_withoutfs)\n",
    "    labels_comparison.append(\"Without FS\")\n",
    "    for name, group in all_comparison_data_grouped:\n",
    "        labels_comparison.append(name[1] + \" (\" + name[0] + \")\")\n",
    "        values_comparison.append(group[\"Testing Score\"])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.boxplot(values_bayopt)\n",
    "    ax1.set_ylabel(\"Score\", size=15)\n",
    "    ax1.set_xticklabels(labels_bayopt)\n",
    "    ax1.set_title(\"BOFS\", fontsize=15)\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax2.boxplot(values_comparison)\n",
    "    ax2.set_xticklabels(labels_comparison)\n",
    "    ax2.set_title(\"Comparison\", fontsize=15)\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "all_bayopt_data_grouped_estimator = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_comparison_data_grouped_estimator = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_withoutfs_data_grouped_estimator = all_withoutfs_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in all_bayopt_data_grouped_estimator:\n",
    "    plot = createScoreComparisonBoxplot(group, all_comparison_data_grouped_estimator.get_group(name), all_withoutfs_data_grouped_estimator.get_group(name))\n",
    "    plot.savefig(comparison_experiment_output_path + \"score_comparison/score_comparison_boxplot_\" + name[0] + \"_\" + name[1] + \".pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score correlation heatmap between BOFS and state-of-the-art approaches\n",
    "def create_comparison_correlation_matrix(bayopt_data, comparison_data, withoutfs_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    grouped_comparison_df = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    labels_bayopt = []\n",
    "    labels_comparison = []\n",
    "    \n",
    "    for name, group in grouped_bayopt_df:\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        labels_bayopt.append(label)\n",
    "        data_dict[label] = scores\n",
    "    for name, group in grouped_comparison_df:\n",
    "        label = name[1] + \" (\" + name[0] + \")\"\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        labels_comparison.append(label)\n",
    "        data_dict[label] = scores\n",
    "    data_dict[\"Without FS\"] = withoutfs_data[\"Testing Score\"].values\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    correlation_df = df.corr().drop(labels_bayopt, axis=0).drop(labels_comparison, axis=1)\n",
    "    \n",
    "    sns.set(style=\"white\", font_scale=1.3)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax = sns.heatmap(data=correlation_df, vmin=0.5, vmax=1, cmap='RdYlGn', center=0.75, annot=True, square=True, cbar=False)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "grouped_bayopt_data = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "grouped_comparison_data = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "grouped_withoutfs_data = all_withoutfs_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in grouped_bayopt_data:\n",
    "    heatmap = create_comparison_correlation_matrix(group, grouped_comparison_data.get_group(name), grouped_withoutfs_data.get_group(name))\n",
    "    figure = heatmap.get_figure()\n",
    "    figure.savefig(comparison_experiment_output_path + \"score_comparison/score_comparison_correlation_heatmap_\" + name[0] + \"_\" + name[1] + \".pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "    sns.set(style=\"white\") # reset font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. BOFS - state-of-the-art Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of runtime of BOFS and state-of-the-art approaches\n",
    "def create_box_plot_of_runtime(bayopt_data, comparison_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    bayopt_durations = []\n",
    "    bayopt_labels = []\n",
    "    grouped_comparison_df = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    comparison_durations = []\n",
    "    comparison_labels = []\n",
    "    for name, group in grouped_bayopt_df:\n",
    "        duration = group[\"Duration Black Box\"] + group[\"Duration Overhead\"]\n",
    "        bayopt_durations.append(duration.values)\n",
    "        bayopt_labels.append(name[1] + \"-\" + name[2] + \"-\" + name[3] + \"-\" + name[4])\n",
    "\n",
    "    for name, group in grouped_comparison_df:\n",
    "        comparison_durations.append(group[\"Duration\"].values)\n",
    "        comparison_labels.append(name[1] + \" (\" + name[0] + \")\")\n",
    "\n",
    "    # Create boxplot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.boxplot(bayopt_durations)\n",
    "    ax1.set_xticklabels(bayopt_labels)\n",
    "    ax1.set_title(\"BOFS Overall Runtime\")\n",
    "    ax1.set_ylabel(\"Runtime [s]\") \n",
    "    ax2.boxplot(comparison_durations)\n",
    "    ax2.set_xticklabels(comparison_labels)\n",
    "    ax2.set_title(\"Comparison Overall Runtime\")\n",
    "    ax2.set_ylabel(\"Runtime [s]\") \n",
    "\n",
    "    # set y axis range to the same for both subplots\n",
    "    y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "    ax1.set_ylim(0, y_max)\n",
    "    ax2.set_ylim(0, y_max)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "runtime_box_plot = create_box_plot_of_runtime(all_bayopt_data, all_comparison_data)\n",
    "runtime_box_plot.savefig(comparison_experiment_output_path + \"runtime_plots/runtime_comparison_box_plot.pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of average runtime of BOFS and state-of-the-art approaches\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_comparison_grouped = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values_black_box = []\n",
    "    values_overhead = []\n",
    "    labels_bay_opt = []\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values_black_box.append(sum(group[\"Duration Black Box\"])/len(group[\"Duration Black Box\"]))\n",
    "        values_overhead.append(sum(group[\"Duration Overhead\"])/len(group[\"Duration Overhead\"]))\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "    \n",
    "    comparison_group = all_comparison_grouped.get_group((estimator, metric))\n",
    "    comparison_data_with_estimator_metric_grouped = comparison_group.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    values_comparison = []\n",
    "    labels_comparison = []\n",
    "    for name, group in comparison_data_with_estimator_metric_grouped:\n",
    "        values_comparison.append(sum(group[\"Duration\"].values)/len(group[\"Duration\"].values))\n",
    "        labels_comparison.append(name[1] + \" (\" + name[0] + \")\")\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.set_title(\"BOFS\")\n",
    "    ax1.bar(labels_bay_opt, values_overhead, label=\"Overhead Runtime\")\n",
    "    ax1.bar(labels_bay_opt, values_black_box, bottom=values_overhead, label=\"Black Box Evaluation Runtime\")\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel(\"Average runtime [s]\")\n",
    "    ax2.set_title(\"Comparison\")\n",
    "    ax2.bar(labels_comparison, values_comparison, label=\"Overall Runtime\")\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel(\"Average runtime [s]\")\n",
    "    # set y axis range to the same for both subplots\n",
    "    y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "    ax1.set_ylim(0, y_max)\n",
    "    ax2.set_ylim(0, y_max)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    fig.savefig(comparison_experiment_output_path + \"runtime_plots/avg_bayesian_divided_runtime_\"+estimator+\"_\"+metric+\".pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "    fig.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayopt_data_with_did = pd.merge(all_bayopt_data, dataset_properties, on=\"did\", how=\"inner\") # merge properties about datasets\n",
    "bayopt_data_with_did_grouped = bayopt_data_with_did.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "\n",
    "for (estimator, metric), group_did in bayopt_data_with_did_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    plt.title(estimator + \"-\" + metric)\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        label = name[1] + \"-\" + name[2] + \"-\" + name[3] + \"-\" + name[4]\n",
    "        group = group.sort_values(by=[\"NumberOfFeatures\"])\n",
    "        plt.plot(group[\"NumberOfFeatures\"], group[\"Duration Black Box\"] + group[\"Duration Overhead\"], \"-o\", label=label)\n",
    "        plt.xlabel(\"Number of Features\") \n",
    "        plt.ylabel(\"Runtime [s]\")\n",
    "        plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "    #plt.savefig(comparison_experiment_output_path + \"runtime_plots/nr_of_features_dependence_\"+estimator+\"_\"+metric+\".eps\", format=\"eps\", bbox_inches = \"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
