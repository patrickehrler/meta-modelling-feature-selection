{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "sns.set(style=\"white\") # reset font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset properties file\n",
    "dataset_properties = pd.read_csv(\"../results/datasets/classification_binary_datasets.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets: Number of features distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = dataset_properties[\"NumberOfFeatures\"].values\n",
    "\n",
    "ax = sns.displot(number_of_features, binwidth=500)\n",
    "ax.set(xlabel='Number of Features', ylabel='Number of datasets')\n",
    "ax.savefig('dataset_feature_distribution.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iteration Experiment Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_experiment_path = \"../results/iteration_number_experiment/\"\n",
    "iter_experiment_output_path = \"iter_experiment_plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all iteration data\n",
    "all_iteration_data = pd.DataFrame()\n",
    "for root, dirs, files in os.walk(iter_experiment_path):\n",
    "    for filename in files:\n",
    "        if filename != \".gitignore\":\n",
    "            all_iteration_data = pd.concat([all_iteration_data, pd.read_csv(iter_experiment_path + filename, skiprows=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAllIterationsTestingScore(name, group):\n",
    "    # group values for each approach\n",
    "    gb = group.groupby([\"Dataset ID\", \"Estimator\", \"Metric\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\"], as_index=False)\n",
    "    grouped_data = [gb.get_group(x) for x in gb.groups]\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 3, figsize=(10,10))\n",
    "    \n",
    "    ax[0,0].set_xlabel('Number of Iterations') \n",
    "    ax[0,0].set_ylabel('Score')\n",
    "\n",
    "    for index in range(0,len(grouped_data)):\n",
    "        if index < 3:\n",
    "            i=0\n",
    "            j=index\n",
    "        elif index < 6:\n",
    "            i=1\n",
    "            j=index-3\n",
    "        else:\n",
    "            i=2\n",
    "            j=index-6\n",
    "            \n",
    "        iteration_labels = grouped_data[index][\"Iteration Steps\"]\n",
    "        testing_values = grouped_data[index][\"Testing Score\"]\n",
    "        training_values = grouped_data[index][\"Training Score\"]\n",
    "        ax[i,j].plot(iteration_labels, testing_values, label = \"Testing\")\n",
    "        ax[i,j].plot(iteration_labels, training_values, label = \"Training\")\n",
    "        title = grouped_data[index].loc[:, \"Learning Method\"].values[0] + \"-\" + grouped_data[index].loc[:, \"Kernel\"].values[0] + \"-\" + grouped_data[index].loc[:, \"Discretization Method\"].values[0]\n",
    "        ax[i,j].set_title(title)\n",
    "        ax[i,j].legend()\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrelationBetweenTestingTraining(name, group):\n",
    "    gb = group.groupby([\"Estimator\", \"Metric\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"Number Features\"], as_index=False)\n",
    "    grouped_data = [gb.get_group(x) for x in gb.groups]\n",
    "    \n",
    "    corr_values = {}\n",
    "    for name, entry in gb:\n",
    "        grouped_entry = entry.groupby([\"Estimator\", \"Metric\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"Iteration Steps\"]).agg({\"Testing Score\": [\"mean\"], \"Training Score\": [\"mean\"]})\n",
    "        label = name[2]+\"-\"+name[3]+\"-\"+name[4]\n",
    "        corr_score = grouped_entry[\"Testing Score\"][\"mean\"].corr(grouped_entry[\"Training Score\"][\"mean\"])\n",
    "        corr_values[label] = corr_score\n",
    " \n",
    "    return pd.DataFrame(corr_values, index=[name[0] + \"-\" + name[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_iteration_data_convergence = all_iteration_data.groupby([\"Dataset ID\", \"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in grouped_iteration_data_convergence:\n",
    "    iter_plot = printAllIterationsTestingScore(name, group)\n",
    "    # save plot to file\n",
    "    filename = str(name[0]) + \"_\" + name[1] + \"_\" + name[2] + \".pdf\"\n",
    "    iter_plot.savefig(iter_experiment_output_path + \"convergence/\"+ filename, format=\"pdf\", bbox_inches = \"tight\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_iteration_data_correlation = all_iteration_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "corr_df = pd.DataFrame()\n",
    "\n",
    "for name, group in grouped_iteration_data_correlation:\n",
    "    corr_df = pd.concat([corr_df, getCorrelationBetweenTestingTraining(name, group)])\n",
    "\n",
    "sns.set(font_scale=2) # font size 2\n",
    "fig, ax = plt.subplots(figsize=(20,4)) \n",
    "ax = sns.heatmap(corr_df, vmin=-1, vmax=1, center=0, annot=True, cmap='RdYlGn', linewidths=1, linecolor=\"black\")\n",
    "fig.autofmt_xdate()\n",
    "filename = \"iter_correlation_plot.pdf\"\n",
    "fig.savefig(iter_experiment_output_path + \"correlation/\" + filename, format=\"pdf\", bbox_inches = \"tight\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the result csv files are stored\n",
    "comparison_experiment_path = \"../results/comparison_bayesian_experiment/classification/\"\n",
    "comparison_experiment_output_path = \"comparison_experiment_plots/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available data\n",
    "dataset_set = set() # set of all dataset id, estimator and metric combinations\n",
    "filename_list = []\n",
    "\n",
    "# explore folder and store all dataset ids and filenames\n",
    "file_list = os.listdir(comparison_experiment_path)\n",
    "for file in file_list:\n",
    "    if file.startswith(\"bayopt\") or file.startswith(\"comparison\") or file.startswith(\"withoutfs\"):\n",
    "        # get properties\n",
    "        dataset_id = file.split(\"_\")[1]\n",
    "        estimator = file.split(\"_\")[2] + \"_\" + file.split(\"_\")[3]\n",
    "        metric = file.split(\"_\")[4].split(\".\")[0]\n",
    "        dataset_set.add((dataset_id, estimator, metric))\n",
    "        filename_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate all bayesian, comparison and withoutfs into one dataframe\n",
    "all_bayopt_data = pd.DataFrame()\n",
    "all_comparison_data = pd.DataFrame()\n",
    "all_withoutfs_data = pd.DataFrame()\n",
    "\n",
    "for dataset_id, estimator, metric in dataset_set:\n",
    "    path_suffix = dataset_id + \"_\" + estimator + \"_\" + metric + \".csv\"\n",
    "    \n",
    "    # import bay opt\n",
    "    bayopt_import= pd.read_csv(comparison_experiment_path + \"bayopt_\" + path_suffix, skiprows=[1])\n",
    "    bayopt_import[\"did\"] = dataset_id\n",
    "    bayopt_import[\"Estimator\"] = estimator\n",
    "    bayopt_import[\"Metric\"] = metric\n",
    "    bayopt_import[\"did\"] = pd.to_numeric(bayopt_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    # import comparison\n",
    "    comparison_import = pd.read_csv(comparison_experiment_path + \"comparison_\" + path_suffix, skiprows=[1])\n",
    "    comparison_import[\"did\"] = dataset_id\n",
    "    comparison_import[\"Estimator\"] = estimator\n",
    "    comparison_import[\"Metric\"] = metric\n",
    "    comparison_import[\"did\"] = pd.to_numeric(comparison_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    # import withoutfs\n",
    "    withoutfs_import = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix)\n",
    "    if withoutfs_import.iloc[0,0] == \"mean\":\n",
    "        withoutfs_import = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix, skiprows=[1])\n",
    "    withoutfs_import[\"did\"] = dataset_id\n",
    "    withoutfs_import[\"Estimator\"] = estimator\n",
    "    withoutfs_import[\"Metric\"] = metric\n",
    "    withoutfs_import[\"did\"] = pd.to_numeric(withoutfs_import[\"did\"]) # convert values to int\n",
    "    \n",
    "    all_bayopt_data = pd.concat([all_bayopt_data, bayopt_import])\n",
    "    all_comparison_data = pd.concat([all_comparison_data, comparison_import])\n",
    "    all_withoutfs_data = pd.concat([all_withoutfs_data, withoutfs_import])\n",
    "\n",
    "# replace strings\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Gaussian Process\", \"GP\")\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Random Forest\", \"RF\")\n",
    "all_bayopt_data = all_bayopt_data.replace(\"Radial Basis Functions\", \"RBF\")\n",
    "all_comparison_data = all_comparison_data.replace(\"Recursive Feature Selection\", \"RFE\")\n",
    "all_comparison_data = all_comparison_data.replace(\"Sequential Feature Selection\", \"SFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_plot_of_scores(bayopt_data, comparison_data, withoutfs_data):\n",
    "    \"\"\" Create plot with Bayesian optimization and comparison scores\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    bayopt_scores = bayopt_data[\"Testing Score\"].values\n",
    "    bayopt_labels = []\n",
    "    for index, row in bayopt_data.iterrows():\n",
    "        if row[\"Learning Method\"] == \"RF\":\n",
    "             bayopt_labels.append(row[\"Learning Method\"] + \" - \" + row[\"Discretization Method\"])\n",
    "        else:\n",
    "            bayopt_labels.append(row[\"Learning Method\"] + \" - \" + row[\"Kernel\"] + \" - \" + row[\"Discretization Method\"])\n",
    "    comparison_scores = comparison_data[\"Testing Score\"].values\n",
    "    comparison_labels = [row[\"Algorithm\"] + \" (\" + row[\"Approach\"] + \")\" for index, row in comparison_data.iterrows()]\n",
    "    withoutfs_score = withoutfs_data[\"Testing Score\"][0]\n",
    "    \n",
    "    ax1.bar(bayopt_labels, bayopt_scores)\n",
    "    ax1.set_title(\"BOFS\")\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.axhline(y=withoutfs_score)\n",
    "    \n",
    "    ax2.bar(comparison_labels, comparison_scores)\n",
    "    ax2.set_title(\"Comparison\")\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.axhline(y=withoutfs_score)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id, estimator, metric in dataset_set:\n",
    "    path_suffix = dataset_id + \"_\" + estimator + \"_\" + metric + \".csv\"\n",
    "    \n",
    "    # create dataframes\n",
    "    bayopt_data = pd.read_csv(comparison_experiment_path + \"bayopt_\" + path_suffix, skiprows=[1])\n",
    "    comparison_data = pd.read_csv(comparison_experiment_path + \"comparison_\" + path_suffix , skiprows=[1])\n",
    "    withoutfs_data = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix)\n",
    "    if withoutfs_data.iloc[0,0] == \"mean\":\n",
    "        withoutfs_data = pd.read_csv(comparison_experiment_path + \"withoutfs_\" + path_suffix, skiprows=[1])\n",
    "    \n",
    "    # replace strings\n",
    "    bayopt_data = bayopt_data.replace(\"Gaussian Process\", \"GP\")\n",
    "    bayopt_data = bayopt_data.replace(\"Random Forest\", \"RF\")\n",
    "    bayopt_data = bayopt_data.replace(\"Radial Basis Functions\", \"RBF\")\n",
    "    comparison_data = comparison_data.replace(\"Recursive Feature Selection\", \"RFE\")\n",
    "    comparison_data = comparison_data.replace(\"Sequential Feature Selection\", \"SFS\")\n",
    "    \n",
    "    # create plot and save file\n",
    "\n",
    "    plot = create_bar_plot_of_scores(bayopt_data, comparison_data, withoutfs_data)\n",
    "    filename = dataset_id + \"_\" + estimator + \"_\" + metric + \".pdf\"\n",
    "    plot.savefig(comparison_experiment_output_path + \"score_comparison/\" + filename, format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAverageScoreBarChart(bayopt_data):\n",
    "    all_bayopt_data_grouped_estimator = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    for name_estimator, group_estimator in all_bayopt_data_grouped_estimator:\n",
    "        all_bayopt_data_grouped = group_estimator.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "        data_dict[name_estimator[0] + \"-\" + name_estimator[1]] = {}\n",
    "        for name, group in all_bayopt_data_grouped:\n",
    "            if name[1] == \"RF\":\n",
    "                label = name[1] + \"-\" + name[3]\n",
    "            else:\n",
    "                label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "            value = sum(group[\"Testing Score\"])/len(group[\"Testing Score\"])\n",
    "            data_dict[name_estimator[0] + \"-\" + name_estimator[1]][label] = value\n",
    "    \n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.bar(data_df.index, data_df[data_df.columns[0]])\n",
    "    ax1.set_title(data_df.columns[0], fontsize=15)\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax2.bar(data_df.index, data_df[data_df.columns[1]])\n",
    "    ax2.set_title(data_df.columns[1], fontsize=15)\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "average_score_chart = createAverageScoreBarChart(all_bayopt_data)\n",
    "average_score_chart.savefig(comparison_experiment_output_path + \"average_score_chart.pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAverageScoreComparisonChart(bayopt_data, comparison_data, withoutfs_data):\n",
    "    data_dict = dict()\n",
    "    all_bayopt_data_grouped = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    all_comparison_data_grouped = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    \n",
    "    value_withoutfs = withoutfs_data[\"Testing Score\"].mean()\n",
    "    values_bayopt = []\n",
    "    labels_bayopt = []\n",
    "    for name, group in all_bayopt_data_grouped:\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bayopt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bayopt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "        values_bayopt.append(group[\"Testing Score\"].mean())\n",
    "    \n",
    "    values_comparison = []\n",
    "    labels_comparison = []\n",
    "    for name, group in all_comparison_data_grouped:\n",
    "        labels_comparison.append(name[1] + \" (\" + name[0] + \")\")\n",
    "        values_comparison.append(group[\"Testing Score\"].mean())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.bar(labels_bayopt, values_bayopt)\n",
    "    ax1.set_title(\"BOFS\", fontsize=15)\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.axhline(y=value_withoutfs)\n",
    "    ax1.tick_params(labelsize=15)\n",
    "    ax2.bar(labels_comparison, values_comparison)\n",
    "    ax2.set_title(\"Comparison\", fontsize=15)\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.axhline(y=value_withoutfs)\n",
    "    ax2.tick_params(labelsize=15)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "all_bayopt_data_grouped_estimator = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_comparison_data_grouped_estimator = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_withoutfs_data_grouped_estimator = all_withoutfs_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in all_bayopt_data_grouped_estimator:\n",
    "    plot = createAverageScoreComparisonChart(group, all_comparison_data_grouped_estimator.get_group(name), all_withoutfs_data_grouped_estimator.get_group(name))\n",
    "    plot.savefig(comparison_experiment_output_path + \"score_comparison_barchart_\" + name[0] + \"_\" + name[1] + \".pdf\", format=\"pdf\", bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorrelationMatrixOnlyBayopt(bayopt_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    for name, group in grouped_bayopt_df:\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        data_dict[label] = scores\n",
    "    \n",
    "    df = pd.DataFrame(data_dict)\n",
    "    correlation_df = df.corr()\n",
    "    \n",
    "    # display only lower diagonal values\n",
    "    mask = np.zeros_like(correlation_df, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    mask[np.diag_indices_from(mask)] = False\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.set(style=\"white\")\n",
    "    ax = sns.heatmap(data=correlation_df, mask=mask, vmin=-1, vmax=1, cmap='RdYlGn', annot=True, square=True, cbar=False)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in all_bayopt_data.groupby([\"Estimator\", \"Metric\"]):\n",
    "    heatmap_only_bayopt = createCorrelationMatrixOnlyBayopt(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_matrix(bayopt_data, comparison_data, withoutfs_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    grouped_comparison_df = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    \n",
    "    data_dict = dict()\n",
    "    \n",
    "    for name, group in grouped_bayopt_df:\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        data_dict[label] = scores\n",
    "    for name, group in grouped_comparison_df:\n",
    "        label = name[1] + \" (\" + name[0] + \")\"\n",
    "        scores = group[\"Testing Score\"].values\n",
    "        data_dict[label] = scores\n",
    "    data_dict[\"Without FS\"] = withoutfs_data[\"Testing Score\"].values\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    correlation_df = df.corr()\n",
    "    \n",
    "    # display only lower diagonal values\n",
    "    mask = np.zeros_like(correlation_df, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    mask[np.diag_indices_from(mask)] = False\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.set(style=\"white\", font_scale=1.0)\n",
    "    ax = sns.heatmap(data=correlation_df, mask=mask, vmin=0.5, vmax=1, cmap='RdYlGn', center=0.75, annot=True, square=True, cbar=False)\n",
    "    \n",
    "    return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_bayopt_data = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "grouped_comparison_data = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "grouped_withoutfs_data = all_withoutfs_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for name, group in grouped_bayopt_data:\n",
    "    heatmap = create_correlation_matrix(group, grouped_comparison_data.get_group(name), grouped_withoutfs_data.get_group(name))\n",
    "    figure = heatmap.get_figure()\n",
    "    figure.savefig(comparison_experiment_output_path + \"score_correlation_heatmap_\" + name[0] + \"_\" + name[1] + \".pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "    sns.set(style=\"white\") # reset font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Overall runtime (BOFS vs Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_box_plot_of_runtime(bayopt_data, comparison_data):\n",
    "    grouped_bayopt_df = bayopt_data.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    bayopt_durations = []\n",
    "    bayopt_labels = []\n",
    "    grouped_comparison_df = comparison_data.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    comparison_durations = []\n",
    "    comparison_labels = []\n",
    "    for name, group in grouped_bayopt_df:\n",
    "        duration = group[\"Duration Black Box\"] + group[\"Duration Overhead\"]\n",
    "        bayopt_durations.append(duration.values)\n",
    "        bayopt_labels.append(name[1] + \"-\" + name[2] + \"-\" + name[3] + \"-\" + name[4])\n",
    "\n",
    "    for name, group in grouped_comparison_df:\n",
    "        comparison_durations.append(group[\"Duration\"].values)\n",
    "        comparison_labels.append(name[1] + \" (\" + name[0] + \")\")\n",
    "\n",
    "    # Create boxplot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.boxplot(bayopt_durations)\n",
    "    ax1.set_xticklabels(bayopt_labels)\n",
    "    ax1.set_title(\"BOFS Overall Runtime\")\n",
    "    ax1.set_ylabel(\"Runtime [s]\") \n",
    "    ax2.boxplot(comparison_durations)\n",
    "    ax2.set_xticklabels(comparison_labels)\n",
    "    ax2.set_title(\"Comparison Overall Runtime\")\n",
    "    ax2.set_ylabel(\"Runtime [s]\") \n",
    "\n",
    "    # set y axis range to the same for both subplots\n",
    "    y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "    ax1.set_ylim(0, y_max)\n",
    "    ax2.set_ylim(0, y_max)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_box_plot = create_box_plot_of_runtime(all_bayopt_data, all_comparison_data)\n",
    "runtime_box_plot.savefig(comparison_experiment_output_path + \"runtime_comparison_box_plot.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "\n",
    "#iteration_box_plot = create_box_plot_of_number_of_iterations(all_bayopt_data)\n",
    "\n",
    "\n",
    "#plot = create_box_plot_of_bayopt_runtime(bayopt_data)\n",
    "#filename = dataset_id + \"_\" + estimator + \"_\" + metric + \".eps\"\n",
    "#plot.savefig(comparison_experiment_output_path + filename, format=\"eps\", bbox_inches = \"tight\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Dependence of runtime to number of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayopt_data_with_did = pd.merge(all_bayopt_data, dataset_properties, on=\"did\", how=\"inner\") # merge properties about datasets\n",
    "bayopt_data_with_did_grouped = bayopt_data_with_did.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "print(all_bayopt_data)\n",
    "print(bayopt_data_with_did)\n",
    "\n",
    "for (estimator, metric), group_did in bayopt_data_with_did_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    plt.title(estimator + \"-\" + metric)\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        label = name[1] + \"-\" + name[2] + \"-\" + name[3] + \"-\" + name[4]\n",
    "        group = group.sort_values(by=[\"NumberOfFeatures\"])\n",
    "        plt.plot(group[\"NumberOfFeatures\"], group[\"Duration Black Box\"] + group[\"Duration Overhead\"], \"-o\", label=label)\n",
    "        plt.xlabel(\"Number of Features\") \n",
    "        plt.ylabel(\"Runtime [s]\")\n",
    "        plt.legend(bbox_to_anchor=(1.0, 1.0))\n",
    "    plt.savefig(comparison_experiment_output_path + \"runtime_plots/nr_of_features_dependence_\"+estimator+\"_\"+metric+\".eps\", format=\"eps\", bbox_inches = \"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 BOFS Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "data = {}\n",
    "\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values_black_box = []\n",
    "    values_overhead = []\n",
    "    labels_bay_opt = []\n",
    "    data_dict = dict()\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values_black_box.append(sum(group[\"Duration Black Box\"])/len(group[\"Duration Black Box\"]))\n",
    "        values_overhead.append(sum(group[\"Duration Overhead\"])/len(group[\"Duration Overhead\"]))\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "    data[estimator + \"_\" + metric] =  (labels_bay_opt, values_black_box, values_overhead)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][2], label=\"Overhead Runtime\")\n",
    "ax1.bar(df[df.columns[0]][0], df[df.columns[0]][1], bottom=df[df.columns[0]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax1.set_ylabel(\"Average runtime [s]\", size=15)\n",
    "ax1.set_title(df.columns[0], size=15)\n",
    "ax1.legend()\n",
    "ax1.tick_params(labelsize=15)\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][2], label=\"Overhead Runtime\")\n",
    "ax2.bar(df[df.columns[1]][0], df[df.columns[1]][1], bottom=df[df.columns[1]][2], label=\"Black Box Evaluation Runtime\")\n",
    "ax2.set_title(df.columns[1], size=15)\n",
    "ax2.legend()\n",
    "ax2.tick_params(labelsize=15)\n",
    "# set y axis range to the same for both subplots\n",
    "y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "ax1.set_ylim(0, y_max)\n",
    "ax2.set_ylim(0, y_max)\n",
    "fig.autofmt_xdate()\n",
    "plt.savefig(comparison_experiment_output_path + \"runtime_plots/bofs_average_runtime.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3. BOFS Division of black-box-runtime and overhead runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of runtime in seconds throughout the different datasets (divided into black box evaluation and overhead runtime)\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "all_comparison_grouped = all_comparison_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values_black_box = []\n",
    "    values_overhead = []\n",
    "    labels_bay_opt = []\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values_black_box.append(sum(group[\"Duration Black Box\"])/len(group[\"Duration Black Box\"]))\n",
    "        values_overhead.append(sum(group[\"Duration Overhead\"])/len(group[\"Duration Overhead\"]))\n",
    "        if name[1] == \"RF\":\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[3])\n",
    "        else:\n",
    "            labels_bay_opt.append(name[1] + \"-\" + name[2] + \"-\" + name[3])\n",
    "    \n",
    "    comparison_group = all_comparison_grouped.get_group((estimator, metric))\n",
    "    comparison_data_with_estimator_metric_grouped = comparison_group.groupby([\"Approach\", \"Algorithm\", \"n_features\"])\n",
    "    values_comparison = []\n",
    "    labels_comparison = []\n",
    "    for name, group in comparison_data_with_estimator_metric_grouped:\n",
    "        values_comparison.append(sum(group[\"Duration\"].values)/len(group[\"Duration\"].values))\n",
    "        labels_comparison.append(name[1] + \" (\" + name[0] + \")\")\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax1.set_title(\"BOFS\")\n",
    "    ax1.bar(labels_bay_opt, values_overhead, label=\"Overhead Runtime\")\n",
    "    ax1.bar(labels_bay_opt, values_black_box, bottom=values_overhead, label=\"Black Box Evaluation Runtime\")\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel(\"Average runtime [s]\")\n",
    "    ax2.set_title(\"Comparison\")\n",
    "    ax2.bar(labels_comparison, values_comparison, label=\"Overall Runtime\")\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel(\"Average runtime [s]\")\n",
    "    # set y axis range to the same for both subplots\n",
    "    y_max = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "    ax1.set_ylim(0, y_max)\n",
    "    ax2.set_ylim(0, y_max)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    fig.savefig(comparison_experiment_output_path + \"runtime_plots/avg_bayesian_divided_runtime_\"+estimator+\"_\"+metric+\".pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "    fig.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4. BOFS number of average iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of number of iterations until convergence (50 iterations without new optimum) throughout the different datasets\n",
    "all_bayopt_grouped = all_bayopt_data.groupby([\"Estimator\", \"Metric\"])\n",
    "\n",
    "res_dict = dict()\n",
    "for (estimator, metric), group_did in all_bayopt_grouped:\n",
    "    bayopt_data_with_estimator_metric_grouped = group_did.groupby([\"Approach\", \"Learning Method\", \"Kernel\", \"Discretization Method\", \"Acquisition Function\", \"n_features\"])\n",
    "    values = []\n",
    "    labels = []\n",
    "    res_dict[estimator + \"-\" + metric] = {}\n",
    "    for name, group in bayopt_data_with_estimator_metric_grouped:\n",
    "        values = group[\"Number of Iterations\"].values\n",
    "        if name[1] == \"RF\":\n",
    "            label = name[1] + \"-\" + name[3]\n",
    "        else:\n",
    "            label = name[1] + \"-\" + name[2] + \"-\" + name[3]\n",
    "        res_dict[estimator + \"-\" + metric][label] = values\n",
    "\n",
    "df = pd.DataFrame(res_dict)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax1.set_title(df.columns[0], size=15)\n",
    "ax1.boxplot(df[df.columns[0]])\n",
    "ax1.set_xticklabels(df.index)\n",
    "ax1.set_ylabel(\"Iterations to convergence\", size=15)\n",
    "ax1.tick_params(labelsize=15)\n",
    "\n",
    "ax2.set_title(df.columns[1], size=15)\n",
    "ax2.boxplot(df[df.columns[1]])\n",
    "ax2.set_xticklabels(df.index)\n",
    "ax2.tick_params(labelsize=15)\n",
    "fig.autofmt_xdate()\n",
    "fig.savefig(comparison_experiment_output_path + \"runtime_plots/boxplot_bayesian_iterations.pdf\", format=\"pdf\", bbox_inches = \"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
